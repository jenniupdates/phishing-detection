{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc7a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas\n",
    "# pip install matplotlib\n",
    "# pip install seaborn\n",
    "# pip install latex\n",
    "# pip install beautifulsoup4\n",
    "# pip install sklearn\n",
    "# pip install numpy\n",
    "# pip install urlextract\n",
    "# pip install wordcloud\n",
    "# pip install scikit-plot\n",
    "# pip install transformers\n",
    "# pip install tensorflow\n",
    "# pip install lxml\n",
    "# pip list\n",
    "# pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125a7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"fraud_email_.csv\")\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "df[\"id\"] = df.index\n",
    "\n",
    "first_column = df.pop('id')\n",
    "df.insert(0, 'id', first_column)\n",
    "df.columns = [x.lower() for x in df.columns] #lowercase headers\n",
    "\n",
    "df.head(5)\n",
    "# class == 1 is fraud email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add test row\n",
    "df.loc[-1] = [-1, 'http://www.google.com http:/ew.bbc.com http://www.facebook.com yomama@gmail.com fakeass@huh', 1]  # adding a row\n",
    "df.index = df.index + 1  # shifting index\n",
    "df.sort_index(inplace=True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text to lower case \n",
    "#import re\n",
    "#df['Text'] = df['Text'].astype(str).apply(lambda x: x.lower()) \n",
    "#df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a85d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove \"=\" symbol from data\n",
    "df['text'] = df['text'].astype(str) # cast as string\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"=\",''))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496385d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,1:2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17781593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3219d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "\n",
    "#extractor = URLExtract()\n",
    "#df1 = df.copy()\n",
    "#df1.Text = df1.Text.apply(extractor.find_urls)\n",
    "#df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "import re\n",
    "\n",
    "# ** ASSUMPTION THAT url be it fake or real, starts with http. if there is fake url that do not start with http,\n",
    "# it is not detected as a url.\n",
    "\n",
    "URLPATTERN = r'(HTTP:|http\\S+)'\n",
    "# urlpattern2 = '(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+' --> cant find a best regex to identify url\n",
    "\n",
    "EMAILPATTERN = r\"[a-zA-Z0-9\\.\\-+_]+@[a-zA-Z0-9\\.\\-+_]+[a-zA-Z0-9\\.\\-+_]+\"\n",
    "\n",
    "df2['urlcount'] = df2.text.apply(lambda x: re.findall(URLPATTERN, x)).str.len()\n",
    "df2['emailcount'] = df2.text.apply(lambda x: re.findall(EMAILPATTERN, x)).str.len()\n",
    "df2['url'] = df2.text.apply(lambda x: re.findall(URLPATTERN, x))\n",
    "df2['email'] = df2.text.apply(lambda x: re.findall(EMAILPATTERN, x))\n",
    "\n",
    "#df2['urls'] = ([int(s) for s in list_nombre.split(' ')]\n",
    "    #[for i in range(d df2.Text.apply(lambda x: re.findall(URLPATTERN, x)).str\n",
    "#df2.groupby('id').sum()['urlcount']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a583d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df2.copy()\n",
    "df4.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.loc[(df4['urlcount'] > 0) & (df4['emailcount'] > 0)]\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0166de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df5['class'],df5.urlcount).plot(kind='bar')\n",
    "plt.title('Fraud Emails')\n",
    "plt.xlabel('Frequency of URLs in Emails')\n",
    "plt.ylabel('Frequency of URLs')\n",
    "\n",
    "pd.crosstab(df5['class'],df5.emailcount).plot(kind='bar')\n",
    "plt.title('Fraud Emails')\n",
    "plt.xlabel('Frequency of Emails in Emails')\n",
    "plt.ylabel('Frequency of Emails')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "urldf = df2[['id', 'url', 'class']]\n",
    "emaildf = df2[['id', 'email', 'class']]\n",
    "emaildf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2 of url and email df only contain rows with email/url data coz dh == unable to understand classification\n",
    "\n",
    "urldf2 = urldf[urldf['url'].map(lambda d: len(d)) > 0]\n",
    "emaildf2 = emaildf[emaildf['email'].map(lambda d: len(d)) > 0]\n",
    "emaildf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate clean and fraud list of emails/url | for own purposes\n",
    "fraudemaillist = []\n",
    "fraudurllist = []\n",
    "\n",
    "def coltolist(dataf, col):\n",
    "    fraudlist = []\n",
    "    cleanlist = []\n",
    "    \n",
    "    for index, row in dataf.iterrows():\n",
    "        if row['class'] == 1:\n",
    "            for rowdata in row[col]:\n",
    "                fraudlist.append(rowdata)\n",
    "        else:\n",
    "            for rowdata in row[col]:\n",
    "                cleanlist.append(rowdata)\n",
    "    return [fraudlist, cleanlist]\n",
    "\n",
    "keke = [urldf2, emaildf2]\n",
    "keke2 = ['url', 'email']\n",
    "tgt = []\n",
    "\n",
    "for i in range(len(keke)):\n",
    "    tgt.append(coltolist(keke[i], keke2[i]))\n",
    "\n",
    "fraudurl = tgt[0][0]\n",
    "cleanurl = tgt[0][1]\n",
    "fraudemail = tgt[1][0]\n",
    "cleanemail = tgt[1][1]\n",
    "\n",
    "cleanemail[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urldf2 , emaildf2 without ids\n",
    "urldf3 = urldf2.drop(['id'], axis=1)\n",
    "emaildf3 = emaildf2.drop(['id'], axis=1)\n",
    "emaildf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudurldf = pd.DataFrame(fraudurl, columns =['url'])\n",
    "fraudurldf['class']=1\n",
    "cleanurldf = pd.DataFrame(cleanurl, columns =['url'])\n",
    "cleanurldf['class']=0\n",
    "fraudemaildf = pd.DataFrame(fraudemail, columns =['email'])\n",
    "fraudemaildf['class']=1\n",
    "cleanemaildf = pd.DataFrame(cleanemail, columns =['email'])\n",
    "cleanemaildf['class']=0\n",
    "cleanurldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [fraudurldf, cleanurldf]\n",
    "url = pd.concat(urls)\n",
    "emails = [fraudemaildf, cleanemaildf]\n",
    "email = pd.concat(emails)\n",
    "email\n",
    "# ************************************ now url and email df contain each email (1 per row) with associated class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dd997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "df3 = df3[['text', 'class']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f57dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot  as plt\n",
    "sns.countplot(x='class', data = df3, palette = 'Pastel1')\n",
    "plt.show()\n",
    "plt.savefig('count_plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f5352",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326530e",
   "metadata": {},
   "source": [
    "## I didn't clean the data by stopwords, punctuation, lowercase, punctuation removals, bad characters etc because tbh I feel fraud messages usually are phrased really badly and thats their characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea450bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29809e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    return text\n",
    "    \n",
    "df3['text'] = df3['text'].apply(clean_text)\n",
    "df3['text'] = df3['text'].str.replace('_',' ')\n",
    "df3.loc[[700]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9b0469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df3.text\n",
    "y = df3['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90620c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba3a2c",
   "metadata": {},
   "source": [
    "# Understanding Term Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa69afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = []\n",
    "for x,y in df3.iterrows():\n",
    "    corpus.append(y['text'])\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(corpus)\n",
    "names = vectorizer.get_feature_names()\n",
    "\n",
    "# http://www.ultravioletanalytics.com/blog/tf-idf-basics-with-pandas-scikit-learn\n",
    "\n",
    "cvec = CountVectorizer(min_df=.0025, max_df=.1)\n",
    "cvec.fit(df.text)\n",
    "cvec_counts = cvec.transform(df.text)\n",
    "\n",
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by=['occurrences'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ba873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frauddf = df[df['class'] == 1]\n",
    "cvec = CountVectorizer(min_df=.0025, max_df=.1)\n",
    "cvec.fit(frauddf.text)\n",
    "cvec_counts = cvec.transform(frauddf.text)\n",
    "fraudterms = cvec.get_feature_names\n",
    "\n",
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "fraudcounts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences_in_fraud': occ})\n",
    "fraudcounts_df.sort_values(by=['occurrences_in_fraud'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9420e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(counts_df,fraudcounts_df,on='term', how='left')\n",
    "resultv2 = result.fillna(0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultv3 = resultv2.copy()\n",
    "# resultv2 takes into consideration NaN values into 0.\n",
    "resultv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultv2['occurrences_in_non_fraud'] = resultv2['occurrences'] - resultv2['occurrences_in_fraud']\n",
    "resultv2[\"word_in_fraud_percentage\"] = resultv2[\"occurrences_in_fraud\"] / resultv2['occurrences'] * 100\n",
    "resultv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c57625",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultv2['word_in_non_fraud_percentage'] = 100 - resultv2['word_in_fraud_percentage']\n",
    "resultv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202670c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized\n",
    "# rationale is beacuse some have more occurrence in fraud resulting in higher chance of getting fraud % higher\n",
    "# so we need to standardize to normalize them to similar standings\n",
    "\n",
    "resultv3['occurrences'] = np.log(resultv3['occurrences'])\n",
    "resultv3['occurrences_in_fraud'] = np.log(resultv3['occurrences_in_fraud'])\n",
    "\n",
    "resultv4 = resultv3.fillna(0)\n",
    "resultv4[\"word_in_fraud_percentage\"] = resultv4[\"occurrences_in_fraud\"] / resultv4['occurrences'] * 100\n",
    "resultv4 = resultv4.fillna(0)\n",
    "resultv4\n",
    "\n",
    "# tbh i lost my thoughts on why i rationalized but the data got some -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(cvec_counts)\n",
    "transformed_weights\n",
    "\n",
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ, 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultv2[['term', 'word_in_fraud_percentage']].sort_values(by='word_in_fraud_percentage', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultv2['occurrences_in_fraud'] = resultv2['occurrences_in_fraud'].apply(np.int64)\n",
    "resultv2['occurrences_in_non_fraud'] = resultv2['occurrences_in_non_fraud'].apply(np.int64)\n",
    "resultv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf707150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too long to run\n",
    "# plt.figure(figsize=(15,10))\n",
    "# resultv2.sort_values(by=\"word_in_fraud_percentage\",ascending=False)[\"word_in_fraud_percentage\"].plot.bar()\n",
    "# plt.xticks(rotation=50)\n",
    "# plt.xlabel(\"Term\")\n",
    "# plt.ylabel(\"Percentage of Term in Fraud Emails\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c979518",
   "metadata": {},
   "source": [
    "# Understanding Term Distribution - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudcorpus = ''\n",
    "nonfraudcorpus = ''\n",
    "\n",
    "for index, row in resultv2.iterrows():\n",
    "    for i in range(row['occurrences_in_fraud']):\n",
    "        fraudcorpus += row['term'] + ' '\n",
    "    for i in range(row['occurrences_in_non_fraud']):\n",
    "        nonfraudcorpus += row['term'] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c36d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfraudcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5dc16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=300, max_words=100, background_color=\"white\", collocations=False).generate(fraudcorpus)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79fe1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"img/fraudword_wordcloud_with_stopwords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b3dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_font_size=300, max_words=100, background_color=\"white\", collocations=False).generate(nonfraudcorpus)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52066df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud.to_file(\"img/non_fraudword_wordcloud_with_stopwords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(stopwords = 'english', max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(fraudcorpus)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf7e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"img/fraudword_wordcloud_without_stopwords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(stopwords = 'english', max_font_size=50, max_words=100, background_color=\"white\", collocations=False).generate(nonfraudcorpus)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize=(14,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db350248",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud.to_file(\"img/non_fraudword_wordcloud_without_stopwords.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = cvec.get_feature_names()\n",
    "# fraud_dic = {words[x]:int(0) for x in range(len(words))}\n",
    "# #list(fraud_dic.items())[:4]\n",
    "# fraud_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ceba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(cvec_counts.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40f7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newname = [x for x in names if not any(c.isdigit() for c in x)]\n",
    "\n",
    "# for x,y in counts_df.iterrows():\n",
    "#     for ch in y.term:\n",
    "#         if any(ch.isdigit()):\n",
    "            \n",
    "\n",
    "# digits = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "# counts_df[~counts_df.term.isin(digits)]\n",
    "# counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9945f26",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79735400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfdc6f9",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# strings to raw docs with tfidf features to floats\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5, max_iter=1000)),\n",
    "               ])\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "coef = logreg.named_steps['clf'].coef_[0]\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07caa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.linear_model\n",
    "# plt.rc('text', usetex=True)\n",
    "\n",
    "# # Retrieve the model parameters.\n",
    "# print(logreg.named_steps['clf'].coef_[0][:2])\n",
    "# w1, w2 = logreg.named_steps['clf'].coef_[0][:2]\n",
    "# # Calculate the intercept and gradient of the decision boundary.\n",
    "# c = -b/w2\n",
    "# m = -w1/w2\n",
    "\n",
    "# # Plot the data and the classification with the decision boundary.\n",
    "# xmin, xmax = -1, 2\n",
    "# ymin, ymax = -1, 2.5\n",
    "# xd = np.array([xmin, xmax])\n",
    "# yd = m*xd + c\n",
    "# plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
    "# plt.fill_between(xd, yd, ymin, color='tab:blue', alpha=0.2)\n",
    "# plt.fill_between(xd, yd, ymax, color='tab:orange', alpha=0.2)\n",
    "\n",
    "# plt.scatter(*X_train[y_train==0].T, s=8, alpha=0.5)\n",
    "# plt.scatter(*X_train[y_train==1].T, s=8, alpha=0.5)\n",
    "# plt.xlim(xmin, xmax)\n",
    "# plt.ylim(ymin, ymax)\n",
    "# plt.ylabel(r'$x_2$')\n",
    "# plt.xlabel(r'$x_1$')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432facd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cdf = pd.DataFrame(LogisticRegression.coef_, X.columns, columns=['Coefficients'])\n",
    "#print(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717534b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd130d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b934c",
   "metadata": {},
   "source": [
    "# Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b8cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3268586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n",
    "from scikitplot.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32185f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df4.copy()\n",
    "    \n",
    "cleaned['text'] = cleaned['text'].apply(clean_text)\n",
    "cleaned['text'] = cleaned['text'].str.replace('_',' ')\n",
    "\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221aede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(row):\n",
    "    text = row['text']\n",
    "    if row['emailcount'] > 0 or row['urlcount'] > 0:\n",
    "        foundurl = row['url']\n",
    "        foundemail = row['email']\n",
    "        for each_url in foundurl:\n",
    "            text = text.replace(each_url, ' ')\n",
    "        for each_email in foundemail:\n",
    "            text = text.replace(each_email, ' ')\n",
    "    return text\n",
    "\n",
    "cleaned['text_cleaned'] = cleaned.apply(func, axis=1)\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09f003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.loc[:,['text_cleaned', 'class']]\n",
    "cleaned.dropna()\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c12f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(cleaned['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7632e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e676c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "\n",
    "# load the pre-trained BERT Tokenizer and Sequence Classifier\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# build our model with the Sequence Classifier and our tokenizer with BERTâ€™s Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb511e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21813b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d0ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned.text_cleaned\n",
    "y = cleaned['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba07621",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputExample(guid=None,\n",
    "             text_a = \"Hello, world\",\n",
    "             text_b = None,\n",
    "             label = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
