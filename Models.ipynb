{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bae07b7-2d85-432c-825d-2c237f4c6008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lim Yin\n",
      "[nltk_data]     Shan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Lim Yin\n",
      "[nltk_data]     Shan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lim Yin\n",
      "[nltk_data]     Shan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cebadbd0-da08-49e1-99fd-c0fb83086e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supply Quality China's EXCLUSIVE dimensions at...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over. SidLet me know. Thx.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Friend,Greetings to you.I wish to accost ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not a surprising assessment from Embassy.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class\n",
       "0  Supply Quality China's EXCLUSIVE dimensions at...      1\n",
       "1                         over. SidLet me know. Thx.      0\n",
       "2  Dear Friend,Greetings to you.I wish to accost ...      1\n",
       "3  MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....      1\n",
       "4          Not a surprising assessment from Embassy.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/file/d/1z3AN8qN3Sz1UTkDp_Bd8pGi4qqb2rXxu/view?usp=sharing'\n",
    "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "df = pd.read_csv(path)\n",
    "df.dropna(inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54585def-41f2-4e96-8a97-e79f107c9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the confusion matrix\n",
    "def plot_cm(y_actual,y_pred):\n",
    "  cf_matrix = confusion_matrix(y_actual,y_pred)\n",
    "\n",
    "  group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "  group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                      cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "  labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "            zip(group_names,group_percentages)]\n",
    "\n",
    "  labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "  ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "  ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "  ax.set_xlabel('\\nPredicted Values')\n",
    "  ax.set_ylabel('Actual Values ');\n",
    "\n",
    "  ## Ticket labels - List must be in alphabetical order\n",
    "  ax.xaxis.set_ticklabels(['False','True'])\n",
    "  ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "  ## Display the visualization of the Confusion Matrix.\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Generate metrics matrix\n",
    "def metrics(actual,pred):\n",
    "  print('accuracy: %s%%' % round(accuracy_score(actual,pred)*100,0))\n",
    "  print('precision: %s%%' % round(precision_score(actual,pred)*100,0))\n",
    "  print('recall: %s%%' % round(recall_score(actual,pred)*100,0))\n",
    "  print('f1_score: %s%%' % round(f1_score(actual,pred)*100,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e22714-85ce-46a0-bc93-f4e821176fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    #Converting the text to lower case \n",
    "    df['Text'] = df['Text'].astype(str).apply(lambda x: x.lower()) \n",
    "    #Remove \"=\" symbol from data\n",
    "    df['Text'] = df['Text'].apply(lambda x: x.replace(\"=\",''))\n",
    "    #Extracting url from the text\n",
    "    df['Url'] = df['Text'].apply(lambda x: re.findall(\"http\\S+\",x))\n",
    "    #Create new feature called Url_Count\n",
    "    df['Url_Count'] = df['Url'].apply(lambda x: len(x))\n",
    "    #Extracting email from the text \n",
    "    df['Email'] = df['Text'].apply(lambda x: re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\",x))\n",
    "    #Create new feature called Email_Count\n",
    "    df['Email_Count'] = df['Email'].apply(lambda x: len(x))\n",
    "    #Removing all symbols from the text except the \"$\" symbol\n",
    "    df['Text'] = df[\"Text\"].apply(lambda x: re.sub('[^a-z$\\s]','',x))\n",
    "    df[\"Text_Length\"] = df[\"Text\"].apply(lambda x: len(x))\n",
    "    \n",
    "    \n",
    "    # #Drop outliers in Url_Count \n",
    "    # range = df['Url_Count'].mean() + df['Url_Count'].std()*3 \n",
    "    # df = df.loc[(df[\"Url_Count\"] <= range)]\n",
    "    # #Drop outliers in Email_Count \n",
    "    # range = df['Email_Count'].mean() + df['Email_Count'].std()*3 \n",
    "    # df = df.loc[(df[\"Email_Count\"] <= range)]\n",
    "    # #Drop outliers in Text_Length\n",
    "    # range = df['Text_Length'].mean() + df['Text_Length'].std()*3 \n",
    "    # df = df.loc[(df[\"Text_Length\"] <= range)]\n",
    "\n",
    "    return df\n",
    "    \n",
    "df = clean_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d17e8d-669b-42d9-ac39-f176289d6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words part 1: Tokenising the text\n",
    "df['Text_Tokens'] = df[\"Text\"].apply(lambda x: word_tokenize(x))\n",
    "# Removing stop words part 2: Removing stopwords\n",
    "def remove_stop_words(word_tokens):\n",
    "    nltk_stop_words = stopwords.words('english')\n",
    "    custom_stop_words = ['.', ',']\n",
    "    combined_stop_words = nltk_stop_words + custom_stop_words\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in combined_stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return (filtered_sentence)\n",
    "df['Text_Filtered'] = df['Text_Tokens'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b454156e-f13c-4311-9d88-e2c4670df76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the list of words\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize(s):\n",
    "    s = [wnl.lemmatize(word) for word in s]\n",
    "    return s\n",
    "df['Text_Filtered_Lemmatized'] = df['Text_Filtered'].apply(\n",
    "    lambda x: lemmatize(x))\n",
    "# Join the word tokens into strings\n",
    "df['Text_Filtered_String'] = df['Text_Filtered_Lemmatized'].apply(\n",
    "    lambda x: ' '.join(x))\n",
    "df['Url_Present'] = df[\"Url_Count\"].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce63df-3557-4779-99b6-f575e217c49a",
   "metadata": {},
   "source": [
    "# TEXT MESSAGE ANALYSIS MODELS\n",
    "only included NB and SVM models here as they are the highest accuracy ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97de16e1-2e18-4839-978c-912d8dddfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to test and train data\n",
    "X = df[\"Text_Filtered_String\"]\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f0e030-898e-4889-87f4-ebde51a136a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.0%\n",
      "precision: 95.0%\n",
      "recall: 98.0%\n",
      "f1_score: 97.0%\n"
     ]
    }
   ],
   "source": [
    "# Model 1 - Multinomial Naive Bayes (With TfidfVectorizer)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_idf = vectorizer.fit_transform(X_train)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_idf, y_train)\n",
    "X_test_idf = vectorizer.transform(X_test)\n",
    "y_pred = nb.predict(X_test_idf)\n",
    "metrics(y_test, y_pred)\n",
    "with open('text_nb.pkl', 'wb') as file:\n",
    "    pickle.dump(nb, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "089ce2a7-80b9-42c3-a821-1612c2f771f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Yin Shan,\\n\\nThank you so much for your confirmation!\\n\\nGood luck with your interviews :)\\n\\nThank you!\\n\\nRegards\\nGrace']\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\"Hi Yin Shan,\n",
    "\n",
    "Thank you so much for your confirmation!\n",
    "\n",
    "Good luck with your interviews :)\n",
    "\n",
    "Thank you!\n",
    "\n",
    "Regards\n",
    "Grace\"\"\"\n",
    "alist = []\n",
    "alist.append(string)\n",
    "print(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be07c32e-1748-49ab-acdd-5e210d4f4ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>Url</th>\n",
       "      <th>Url_Count</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_Count</th>\n",
       "      <th>Text_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi yin shan\\n\\nthank you so much for your conf...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class Url  Url_Count  \\\n",
       "0  hi yin shan\\n\\nthank you so much for your conf...      0  []          0   \n",
       "\n",
       "  Email  Email_Count  Text_Length  \n",
       "0    []            0          111  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = {'Text':[string], 'Class': [0]}\n",
    "test_df = pd.DataFrame(data=test_df)\n",
    "test_df = clean_text(test_df)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74630f2c-655c-4c4b-9ba1-485fb49b9db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
